{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6defee19-2f50-4cb6-b762-3c166c708bfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Feb 19 13:10:50 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla V100-SXM2-32GB           On  |   00000000:1A:00.0 Off |                    0 |\n",
      "| N/A   34C    P0             39W /  300W |       0MiB /  32768MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  Tesla V100-SXM2-32GB           On  |   00000000:1C:00.0 Off |                    0 |\n",
      "| N/A   32C    P0             40W /  300W |       0MiB /  32768MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   2  Tesla V100-SXM2-32GB           On  |   00000000:1D:00.0 Off |                    0 |\n",
      "| N/A   32C    P0             41W /  300W |       0MiB /  32768MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   3  Tesla V100-SXM2-32GB           On  |   00000000:1E:00.0 Off |                    0 |\n",
      "| N/A   34C    P0             42W /  300W |       0MiB /  32768MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2024 NVIDIA Corporation\n",
      "Built on Thu_Mar_28_02:18:24_PDT_2024\n",
      "Cuda compilation tools, release 12.4, V12.4.131\n",
      "Build cuda_12.4.r12.4/compiler.34097967_0\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi\n",
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63cbfbec-5d43-43e8-a6a6-03ba03bab708",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "220d0510-0a91-4e01-a95d-92864a354928",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"..\")  # Move to the project root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "113cef89-6c47-4393-b3a5-5bb1aa60d3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from tqdm import tqdm\n",
    "from utils.io_utils import IOHandler, DebiasedOutput\n",
    "from main import MultiLLMDebiasing\n",
    "from prompts import get_feedback_prompt\n",
    "# Mocking IOHandler and MultiLLMDebiasing for demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9cb73373-61af-424f-97ac-a10156cab72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "harm_assignments_file=\"harm_assignments.yaml\"\n",
    "input_file=\"/home/sklioui/captions/samples.csv\"\n",
    "output_file=\"/home/sklioui/captions/debiased_samples_wtf.json\"\n",
    "max_rounds=3\n",
    "temperature=0.0\n",
    "return_lineage=True\n",
    "include_metadata=True\n",
    "max_new_tokens=512\n",
    "temperature=0.0\n",
    "return_lineage=True\n",
    "return_feedback=True,\n",
    "include_metadata=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd0f50e7-0be3-455d-a3fe-aa96baf30153",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate argparse.Namespace for compatibility\n",
    "args = argparse.Namespace(\n",
    "    harm_assignments=harm_assignments_file,\n",
    "    input_file=input_file,\n",
    "    output_file=output_file,\n",
    "    max_rounds=max_rounds,\n",
    "    max_new_tokens=max_new_tokens,\n",
    "    temperature=temperature,\n",
    "    return_lineage=return_lineage,\n",
    "    return_feedback=return_feedback,\n",
    "    include_metadata=include_metadata\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c77ea70-91ae-4162-a5db-29889d6c0a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process harm assignments\n",
    "harm_assignments, strategy = IOHandler.process_harm_assignments(args.harm_assignments)\n",
    "\n",
    "# Load queries\n",
    "queries = IOHandler.load_queries(args.input_file)\n",
    "\n",
    "config = {\n",
    "    'max_rounds': args.max_rounds,\n",
    "    'max_new_tokens': args.max_new_tokens,\n",
    "    'temperature': args.temperature\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "048411b6-c664-4ed8-9d45-fea786e22071",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Qwen/Qwen2.5-14B-Instruct': [],\n",
       " 'tiiuae/Falcon3-10B-Instruct': ['DEROGATORY',\n",
       "  'DISPARATE_PERFORMANCE',\n",
       "  'ERASURE',\n",
       "  'EXCLUSIONARY',\n",
       "  'MISREPRESENTATION',\n",
       "  'STEREOTYPING',\n",
       "  'TOXICITY',\n",
       "  'DIRECT_DISCRIMINATION',\n",
       "  'INDIRECT_DISCRIMINATION'],\n",
       " 'meta-llama/Llama-3.1-8B-Instruct': ['DEROGATORY',\n",
       "  'DISPARATE_PERFORMANCE',\n",
       "  'ERASURE',\n",
       "  'EXCLUSIONARY',\n",
       "  'MISREPRESENTATION',\n",
       "  'STEREOTYPING',\n",
       "  'TOXICITY',\n",
       "  'DIRECT_DISCRIMINATION',\n",
       "  'INDIRECT_DISCRIMINATION'],\n",
       " 'unsloth/phi-4': ['DEROGATORY',\n",
       "  'DISPARATE_PERFORMANCE',\n",
       "  'ERASURE',\n",
       "  'EXCLUSIONARY',\n",
       "  'MISREPRESENTATION',\n",
       "  'STEREOTYPING',\n",
       "  'TOXICITY',\n",
       "  'DIRECT_DISCRIMINATION',\n",
       "  'INDIRECT_DISCRIMINATION']}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "harm_assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fcc25630-9f6c-4423-b5b3-3f25ce89b246",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-19 13:16:19,464 - main - INFO - Initializing MultiLLMDebiasing with strategy: centralized\n",
      "2025-02-19 13:16:19,467 - main - INFO - Loading model: Qwen/Qwen2.5-14B-Instruct\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found token file at: config/hf_token.yml\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ed383ab48b245acbf199841b3fd0020",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-19 13:20:30,540 - main - INFO - Assigned harm types for Qwen/Qwen2.5-14B-Instruct: set()\n",
      "2025-02-19 13:20:30,542 - main - INFO - Loading model: tiiuae/Falcon3-10B-Instruct\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found token file at: config/hf_token.yml\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b805c030cc3445c96001ceba9547c0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-19 13:23:02,899 - main - INFO - Assigned harm types for tiiuae/Falcon3-10B-Instruct: {'ERASURE', 'STEREOTYPING', 'EXCLUSIONARY', 'DEROGATORY', 'DIRECT_DISCRIMINATION', 'INDIRECT_DISCRIMINATION', 'MISREPRESENTATION', 'TOXICITY', 'DISPARATE_PERFORMANCE'}\n",
      "2025-02-19 13:23:02,901 - main - INFO - Loading model: meta-llama/Llama-3.1-8B-Instruct\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found token file at: config/hf_token.yml\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1d1eb3b8c724079867ce335a6ae5f6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-19 13:24:54,887 - main - INFO - Assigned harm types for meta-llama/Llama-3.1-8B-Instruct: {'ERASURE', 'STEREOTYPING', 'EXCLUSIONARY', 'DEROGATORY', 'DIRECT_DISCRIMINATION', 'INDIRECT_DISCRIMINATION', 'MISREPRESENTATION', 'TOXICITY', 'DISPARATE_PERFORMANCE'}\n",
      "2025-02-19 13:24:54,889 - main - INFO - Loading model: unsloth/phi-4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found token file at: config/hf_token.yml\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9008d74a14444c2a9f18f475fb6f268",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-19 13:27:50,489 - main - INFO - Assigned harm types for unsloth/phi-4: {'ERASURE', 'STEREOTYPING', 'EXCLUSIONARY', 'DEROGATORY', 'DIRECT_DISCRIMINATION', 'INDIRECT_DISCRIMINATION', 'MISREPRESENTATION', 'TOXICITY', 'DISPARATE_PERFORMANCE'}\n",
      "2025-02-19 13:27:50,491 - main - INFO - Successfully initialized centralized reducer\n"
     ]
    }
   ],
   "source": [
    "debiasing = MultiLLMDebiasing(\n",
    "        harm_assignments=harm_assignments,\n",
    "        config=config,\n",
    "        strategy=strategy\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d5d5e17e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sklioui/miniconda3/envs/luma_unbias/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/sklioui/miniconda3/envs/luma_unbias/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/sklioui/miniconda3/envs/luma_unbias/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:650: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "Processing queries: 100%|██████████| 3/3 [06:33<00:00, 131.15s/it]\n"
     ]
    }
   ],
   "source": [
    "# Process queries and collect outputs\n",
    "outputs = []\n",
    "for i, query in tqdm(enumerate(queries), desc=\"Processing queries\", total=len(queries)):\n",
    "    try:\n",
    "        result = debiasing.get_debiased_response(\n",
    "            query, \n",
    "            args.return_lineage, \n",
    "            args.return_feedback\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing query {i}: {e}\")\n",
    "        continue    \n",
    "\n",
    "    metadata = {\"query_index\": i} if args.include_metadata else None\n",
    "\n",
    "    output = DebiasedOutput(\n",
    "        original_query=query,\n",
    "        debiased_response=result.final_response,\n",
    "        lineage=result.lineage,\n",
    "        feedback=result.feedback,\n",
    "        metadata=metadata\n",
    "    )\n",
    "    outputs.append(output)\n",
    "\n",
    "# Save results\n",
    "IOHandler.save_outputs(outputs, args.output_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "luma_unbias",
   "language": "python",
   "name": "luma_unbias"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
